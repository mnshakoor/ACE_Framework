{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qWbh0Ue1Lr8uYlCrjdGgjkGZwR7fIx8i",
      "authorship_tag": "ABX9TyNKD3TTO/zvdJ9ME+ppYs25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnshakoor/ACE_Framework/blob/main/ARAC_ContentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uajxo3iPHGo",
        "outputId": "8d333688-4d2d-4c9b-cb89-2212a9b33534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to files.\n"
          ]
        }
      ],
      "source": [
        "# Define the path to your file in Google Drive\n",
        "# Replace \"Colab Files/my_analysis_file.pdf\" with the path to your actual file\n",
        "file_path = \"/content/drive/MyDrive/Colab/Conflict_In_Cameroon.pdf\"\n",
        "\n",
        "# Step 1: Download required NLTK resources for TextBlob\n",
        "import nltk\n",
        "\n",
        "# Download all necessary data for TextBlob\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('brown')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # This line was added to download the missing resource\n",
        "\n",
        "# Specifically download the 'averaged_perceptron_tagger_eng' resource\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the missing resource\n",
        "\n",
        "\n",
        "# Step 2: Proceed with other imports and script setup\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from collections import defaultdict\n",
        "import fitz  # PyMuPDF for PDFs\n",
        "from docx import Document  # For Word files\n",
        "import json\n",
        "\n",
        "# Load spaCy English model with dependency parsing and NER capabilities\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize transformers sentiment analysis pipeline\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Function to read text from different file formats\n",
        "def read_text_from_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        text = \"\"\n",
        "        with fitz.open(file_path) as pdf:\n",
        "            for page in pdf:\n",
        "                text += page.get_text()\n",
        "    elif file_path.endswith('.docx'):\n",
        "        doc = Document(file_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please use .txt, .pdf, or .docx files.\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to perform content analysis\n",
        "def content_analysis(text):\n",
        "    doc = nlp(text)\n",
        "    words = [token.text for token in doc if token.is_alpha]\n",
        "    word_freq = pd.Series(words).value_counts()\n",
        "    blob = TextBlob(text)\n",
        "    emotional_words = [word for word, pos in blob.tags if pos in ['JJ', 'RB']]\n",
        "    return word_freq.head(10).to_dict(), emotional_words\n",
        "\n",
        "# ... (previous code) ...\n",
        "\n",
        "# Function to perform sentiment analysis with truncation\n",
        "def sentiment_analysis_func(text, max_length=512):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    # Truncate the text for transformer sentiment analysis\n",
        "    truncated_text = text[:max_length]  # Truncate to the maximum allowed length\n",
        "    transformer_sentiment = sentiment_analysis(truncated_text)\n",
        "    return polarity, subjectivity, transformer_sentiment\n",
        "\n",
        "# ... (rest of the code) ...\n",
        "\n",
        "# Function to perform topic modeling\n",
        "def topic_modeling(text):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform([text])\n",
        "    lda = LatentDirichletAllocation(n_components=1, random_state=42)\n",
        "    lda.fit(X)\n",
        "    topic_words = {f\"Topic {i}\": [vectorizer.get_feature_names_out()[j] for j in topic.argsort()[:-11:-1]]\n",
        "                   for i, topic in enumerate(lda.components_)}\n",
        "    return topic_words\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER)\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = defaultdict(list)\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.label_].append(ent.text)\n",
        "    return dict(entities)\n",
        "\n",
        "# Function for dependency parsing to detect metaphors and framing structures\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    metaphors = []\n",
        "    frames = []\n",
        "    for sent in doc.sents:\n",
        "        if \" like \" in sent.text or \" as \" in sent.text:\n",
        "            metaphors.append(sent.text)\n",
        "        for token in sent:\n",
        "            if token.lemma_ in [\"freedom\", \"control\"]:\n",
        "                frames.append(sent.text)\n",
        "                break\n",
        "    return metaphors, frames\n",
        "\n",
        "# Function to save results to files\n",
        "def save_results_to_files(results, output_prefix=\"analysis_results\"):\n",
        "    # Save as a text file\n",
        "    with open(f\"{output_prefix}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "        for section, content in results.items():\n",
        "            file.write(f\"=== {section} ===\\n\")\n",
        "            file.write(f\"{content}\\n\\n\")\n",
        "\n",
        "    # Save as a CSV file (for tabular data like word frequencies and entities)\n",
        "    pd.DataFrame.from_dict(results[\"Word Frequency\"], orient='index', columns=[\"Frequency\"]).to_csv(f\"{output_prefix}_word_frequency.csv\")\n",
        "    pd.DataFrame.from_dict(results[\"Named Entities\"], orient='index').to_csv(f\"{output_prefix}_entities.csv\")\n",
        "\n",
        "    # Save as a JSON file for structured data\n",
        "    with open(f\"{output_prefix}.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(results, file, indent=4)\n",
        "\n",
        "# Running all analyses\n",
        "def run_cognitive_linguistic_analysis(file_path):\n",
        "    # Read text from file\n",
        "    text = read_text_from_file(file_path)\n",
        "\n",
        "    # Perform analyses\n",
        "    word_freq, emotional_words = content_analysis(text)\n",
        "    polarity, subjectivity, transformer_sentiment = sentiment_analysis_func(text)\n",
        "    topics = topic_modeling(text)\n",
        "    entities = named_entity_recognition(text)\n",
        "    metaphors, frames = dependency_parsing(text)\n",
        "\n",
        "    # Compile results into a dictionary\n",
        "    results = {\n",
        "        \"Word Frequency\": word_freq,\n",
        "        \"Emotional Words\": emotional_words,\n",
        "        \"Polarity\": polarity,\n",
        "        \"Subjectivity\": subjectivity,\n",
        "        \"Transformer Sentiment\": transformer_sentiment,\n",
        "        \"Topics\": topics,\n",
        "        \"Named Entities\": entities,\n",
        "        \"Metaphors\": metaphors,\n",
        "        \"Frames\": frames\n",
        "    }\n",
        "\n",
        "    # Save results to files\n",
        "    save_results_to_files(results)\n",
        "    print(\"Results saved to files.\")\n",
        "\n",
        "# Specify the path to your file (PDF, DOCX, or TXT)\n",
        "file_path = \"/content/drive/MyDrive/Colab/Conflict_In_Cameroon.pdf\"  # Replace with the path to your file\n",
        "\n",
        "# Run the analysis on the file\n",
        "run_cognitive_linguistic_analysis(file_path)\n"
      ]
    },
    {
      "source": [
        "!pip install -U textblob\n",
        "!python -m textblob.download_corpora"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZwHGEo14696",
        "outputId": "1c0f3749-31e8-42e7-eede-5439cc2b2a26"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.18.0.post0)\n",
            "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (4.66.6)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Define the path to your file in Google Drive\n",
        "# Replace \"Colab Files/my_analysis_file.pdf\" with the path to your actual file\n",
        "file_path = \"/content/drive/MyDrive/Colab/Conflict_In_Cameroon.pdf\"\n",
        "\n",
        "# Step 1: Download required NLTK resources for TextBlob\n",
        "import nltk\n",
        "\n",
        "\n",
        "# Download all necessary data for TextBlob\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('brown')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('universal_tagset')\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # This line was added to download the missing resource\n",
        "\n",
        "\n",
        "# Step 2: Proceed with other imports and script setup\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from collections import defaultdict\n",
        "import fitz  # PyMuPDF for PDFs\n",
        "from docx import Document  # For Word files\n",
        "import json\n",
        "\n",
        "# ... (rest of the code remains the same)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyxHjrmC5C_X",
        "outputId": "b244005f-9e72-4896-804a-fbeead31e137"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "ju1M7PoCuljg",
        "outputId": "f1a2ac73-dca4-4e8f-f821-ae0bfb378385"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-13-bec4ba3f7ac1>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-bec4ba3f7ac1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python -m textblob.download_corpora\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5jaGgk22m_w",
        "outputId": "b4d95069-9ff4-4cfd-9821-00137939d854"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y textblob\n",
        "!pip install textblob\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX6XSQWGRyTB",
        "outputId": "3bfe8dcf-5d37-4916-8814-1217543b940a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: textblob 0.17.1\n",
            "Uninstalling textblob-0.17.1:\n",
            "  Successfully uninstalled textblob-0.17.1\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob) (4.66.6)\n",
            "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: textblob\n",
            "Successfully installed textblob-0.18.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxzBFwjiS2yX",
        "outputId": "fac2a192-fcdb-4c5e-ae94-8c6454087117"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "ea68nuFEWObx"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}